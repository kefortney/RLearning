---
title: "R Notebook"
output: 
  flexdashboard::flex_dashboard:
    vertical_layout: scroll
---
### Machine Learning with Burlington Housing Data
The city of Burlington in Vermont keeps an open dataset for anyone to explore. In this case the goal is to add sale prices for those that do not have it ith machine learning and then decided where I want to explore the data further.
```{r}
rm(list=ls())
library(ggplot2) # visualization
library(scales) # visualization
library(dplyr) # data manipulation
library(mice) # imputation
library(randomForest) # classification algorithm
library(flexdashboard)
```
Verify the Structure of the dataframe
```{r}
wholedf <- read.csv("C:/Users/kefor/Desktop/RLearning/City_of_Burlington_Property_Details.csv", header = TRUE)
dim(wholedf)
```
### Data Structure
First it makes sense to take a look at the structure of the data and see what kind of variables it includes.
```{r}
str(wholedf)
```
### Null Values
See how many Null values are in the dataset and if anything needs to be filled in.
```{r}
colSums(sapply(wholedf, is.na))
```
### Residential
Cut the data down to just include certain subsets which are just residential properties
```{r}
wholedf <- subset(wholedf, LandUse == "Single Family" | LandUse == "2 Family" | LandUse == "3 Family"| LandUse == "4 Family" | LandUse == "Apartments 5+Units")
#wholedf <- subset(wholedf, LandUse == "Single Family" | LandUse == "2 Family" | LandUse == "3 Family" | LandUse == "4 Family" | LandUse == "Apartments 5+Units" | LandUse == "Condo")
```
### Remove Features
Some of the data in this dataframe is not going to be useful in figuring out the saleprice.  For those we are going to remove what we don't need. Dim allows you to verify that the number of columns has dropped
```{r}
(wholedf <- wholedf[,c(1,2,4,6,8,9,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,31,32,33,34,35,36,37,38)])
dim(wholedf)
```
### Remove SalePrice Outliers
For some of the SalePrice values there are zeros, which in this case refers to missing data.  A quick function replaces zeros with "NA" so those values will be automatically discarded when plotted and allow us to seperate it later when it come to splitting it into a training and a testing dataset.

```{r}
summary(wholedf$SalePrice)
wholedf$SalePrice[1:30]
wholedf$SalePrice[wholedf$SalePrice == 0] <- NA

require(scales)
qplot(SalePrice, data=wholedf, geom="histogram",bins=800) + 
  scale_x_continuous(labels = dollar)
```
### Sale Price Boundries
Again outliers are a problem so removing the upper edge is useful in our analysis. Perhaps after running this whole thing I may end up going back to cut out more than 2 Family houses to get a better predictive model for what is interesting to me.
```{r}
wholedf <- subset(wholedf, wholedf$SalePrice < 2000000 | is.na(wholedf$SalePrice) )
```
```{r}
wholedf$Grade[wholedf$Grade == 'AVERAGE PLUS' | wholedf$Grade == 'AVERAGEMINUS'] <- 'AVERAGE'
wholedf$Grade[wholedf$Grade == 'EXCLT PLUS' | wholedf$Grade == 'EXCLNT MINUS' ] <- 'EXCELLENT'
wholedf$Grade[wholedf$Grade == 'FAIR PLUS' | wholedf$Grade == 'FAIR MINUS' ] <- 'FAIR'
wholedf$Grade[wholedf$Grade == 'GOOD PLUS' | wholedf$Grade == 'GOOD MINUS' ] <- 'GOOD'
wholedf$Grade[wholedf$Grade == 'VRYGOODPLUS' | wholedf$Grade == 'VRYGOODMINUS' ] <- 'VERY GOOD'
wholedf$Grade[wholedf$Grade == 'CUSTOM MINUS' | wholedf$Grade == 'CUSTOM PLUS'| wholedf$Grade == 'CUSTOM'] <- 'AVERAGE'
wholedf$Grade[wholedf$Grade == 'POOR PLUS' | wholedf$Grade == 'POOR MINUS'] <- 'POOR'


```



```{r}
ggplot(wholedf, aes(x = LandUse, y = SalePrice, fill = factor(Grade))) +
  geom_boxplot() +
  scale_y_continuous(labels=dollar_format()) 
```

### Feature Engineering
Time to make some new features that will be useful in determing the sale price
```{r}
# Create a new variable
wholedf$PercentFinished <- wholedf$FinishedArea / wholedf$TotalGrossArea
wholedf$PercentBuildingtoTotal <- wholedf$CurrentBuildingValue / wholedf$CurrentValue
```

```{r}
ggplot(wholedf, aes(Grade, fill = LandUse)) +
  geom_bar()
```

```{r}
ggplot(wholedf, aes(PercentFinished, SalePrice, colour = LandUse)) + 
  geom_point() +
  scale_x_continuous(labels = comma) +
  scale_y_continuous(labels = dollar) +
  facet_grid(.~LandUse)  
```
```{r}
wholedf$Grade <- factor(wholedf$Grade)
wholedf$Grade
```

### Create Train/Test Datasets
Split the dataset by if there is a price in SalePrice or not
```{r}
train <- subset(wholedf, wholedf$SalePrice > 0 )
test <- subset(wholedf, is.na(wholedf$SalePrice) )
```

```{r}
# Set a random seed
set.seed(800)

# Build the model (note: not all possible variables are used)
rf_model <- randomForest(factor(SalePrice) ~ CurrentAcres + TotalGrossArea + FinishedArea + PercentFinished + NumofRooms + NumofBedrooms + Depreciation + Grade, data = train)

```


```{r}
# Get importance
importance    <- importance(rf_model)
varImportance <- data.frame(Variables = row.names(importance), 
                            Importance = round(importance[ ,'MeanDecreaseGini'],2))

# Create a rank variable based on importance
rankImportance <- varImportance %>%
  mutate(Rank = paste0('#',dense_rank(desc(Importance))))

# Use ggplot2 to visualize the relative importance of variables
ggplot(rankImportance, aes(x = reorder(Variables, Importance), 
    y = Importance, fill = Importance)) +
  geom_bar(stat='identity') + 
  geom_text(aes(x = Variables, y = 0.5, label = Rank),
    hjust=0, vjust=0.55, size = 4, colour = 'red') +
  labs(x = 'Variables') +
  coord_flip() 
```

```{r}
# Predict using the test set
prediction <- predict(rf_model, test)
```

```{r}
test$SalePrice <- prediction
full  <- rbind(train, test)
full
```

```{r}
preProcess=c("center", "scale")
```
```{r}
# Linear Discriminant Analysis
set.seed(seed)
fit.lda <- train(diabetes~., data=dataset, method="lda", metric=metric, preProc=c("center", "scale"), trControl=control)
# Logistic Regression
set.seed(seed)
fit.glm <- train(diabetes~., data=dataset, method="glm", metric=metric, trControl=control)
# GLMNET
set.seed(seed)
fit.glmnet <- train(diabetes~., data=dataset, method="glmnet", metric=metric, preProc=c("center", "scale"), trControl=control)
# SVM Radial
set.seed(seed)
fit.svmRadial <- train(diabetes~., data=dataset, method="svmRadial", metric=metric, preProc=c("center", "scale"), trControl=control, fit=FALSE)
# kNN
set.seed(seed)
fit.knn <- train(diabetes~., data=dataset, method="knn", metric=metric, preProc=c("center", "scale"), trControl=control)
# Naive Bayes
set.seed(seed)
fit.nb <- train(diabetes~., data=dataset, method="nb", metric=metric, trControl=control)
# CART
set.seed(seed)
fit.cart <- train(diabetes~., data=dataset, method="rpart", metric=metric, trControl=control)
# C5.0
set.seed(seed)
fit.c50 <- train(diabetes~., data=dataset, method="C5.0", metric=metric, trControl=control)
# Bagged CART
set.seed(seed)
fit.treebag <- train(diabetes~., data=dataset, method="treebag", metric=metric, trControl=control)
# Random Forest
set.seed(seed)
fit.rf <- train(diabetes~., data=dataset, method="rf", metric=metric, trControl=control)
# Stochastic Gradient Boosting (Generalized Boosted Modeling)
set.seed(seed)
fit.gbm <- train(diabetes~., data=dataset, method="gbm", metric=metric, trControl=control, verbose=FALSE)
You can see a good mixt
```
```{r}
results <- resamples(list(lda=fit.lda, logistic=fit.glm, glmnet=fit.glmnet,
	svm=fit.svmRadial, knn=fit.knn, nb=fit.nb, cart=fit.cart, c50=fit.c50,
	bagging=fit.treebag, rf=fit.rf, gbm=fit.gbm))
# Table comparison
summary(results)
```

```{r}


# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
1
2
3
4
# boxplot comparison
bwplot(results)
# Dot-plot comparison
dotplot(results)
```

```{r}
#wholedf <- subset(wholedf, c("AccountNumber","ParcelID","StreetNumber","StreetName","Unit","CuO1LastName","CuO1FirstName","LandUse","CurrentAcres","TotalGrossArea","FinishedArea","CurrentValue","CurrentLandValue","CurrentYardItemsValue","CurrentBuildingValue","BuildingType","HeatFuel","HeatType","Grade","YearBlt","SaleDate","SalePrice","NumofRooms","NumofBedrooms","Baths","NumofUnits","ZoningCode","Foundation","Depreciation","PropertyCenterPoint"))
```

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
